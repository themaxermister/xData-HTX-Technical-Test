# HTX XData Technical Test

This project contains a spark-based transformation code, using only RDD on Scala.
This documentation will guide you through the project setup and describe the thought process behind the code.

## Setting up the project

### Prerequisites

- Scala 2.12.15
- sbt 1.9.9
- Java 8
- Spark 3.5.1

### Running the code

To run the code, you can use the following command:

```scala Main <input_path_1> <input_path_2> <output_path_3> <top_n_value>```

- input_path_1: The path to the parquet file containing Dataset A
- input_path_2: The path to the parquet file containing Dataset B
- output_path_3: The path to the output file where the result will be stored
- top_n_value: The number of top items to be displayed

## Datasets

Datasets that are generated by the tests are located in `src/test/resources`. You are recommended to store your dataset files in `src/main/resources`

### Dataset A: Detection Records

Table size: approximately ~1 million rows

| Column Name               	| Column Type   	| Comment                                                                          	|
|---------------------------	|---------------	|----------------------------------------------------------------------------------	|
| geographical_location_oid 	| bigint        	| A unique bigint identifier for the geographical location                         	|
| video_camera_oid          	| bigint        	| A unique bigint identifier for the video camera that the item was detected from. 	|
| detection_oid             	| bigint        	| A unique bigint identifier for each detection event.                             	|
| item_name                 	| varchar(5000) 	| Item name                                                                        	|
| timestamp_detected        	| bigint        	| timestamp for a given timestamp detected                                         	|

### Dataset B: Location Records

Table size: approximately ~10,000 rows

| Column Name               	| Column Type  	| Comment                                                  	|
|---------------------------	|--------------	|----------------------------------------------------------	|
| geographical_location_oid 	| bigint       	| A unique bigint identifier for the geographical location 	|
| geographical_location     	| varchar(500) 	| Geographical Location                                    	|

### Dataset C: Item Rank Records

Table size: Depends on the limit set by the user

| Column Name          	| Column Type   	| Comment                                                                              	|
|----------------------	|---------------	|--------------------------------------------------------------------------------------	|
| geograhical_location 	| bigint        	| A unique bigint identifier for the geographical location                             	|
| item_rank            	| varchar(500)  	| Item_rank = 1 corresponds to the most popular item detected in geographical_location 	|
| item_name            	| varchar(5000) 	| Item name                                                                            	|

## Thought Process

The code was built as several parts:

1. Spark configuration
2. Generating the sample datasets
3. Spark functions (reading and writing the datasets)
4. Transforming the datasets
5. Utilities functions
6. Testing the code

### 1. Spark Configuration

```scala
 val sparkConf = new SparkConf()
      .setMaster("local[2]")
      .setAppName("htx_spark_test")
      .set("spark.executor.memory", "4g") // Set executor memory
      .set("spark.driver.memory", "2g") // Set driver memory
      .set("spark.sql.shuffle.partitions", "4") // Set number of shuffle partitions
```
The `setMaster` specifies the cluster manager to use. Here, `local[2]` indicates that the Spark application runs in local mode with 2 CPU cores. Local mode is suitable for development and testing purposes. The number inside the square brackets [2] represents the number of CPU cores allocated to Spark. 

The `spark.executor.memory` configuration sets the amount of memory allocated to each executor in gigabytes. Executors are worker processes in Spark responsible for executing tasks. In this case, each executor is allocated 4 GB of memory. You should adjust this value based on your workload and the memory availability of the cluster. 

The `spark.driver.memory` sets the amount of memory allocated to the driver process in gigabytes. The driver is the main control program for the Spark application, responsible for orchestrating the execution of tasks and maintaining the overall state of the application. Here, the driver is allocated 2 GB of memory. Again, adjust this value based on the application's requirements and available resources.

The `spark.sql.shuffle.partitions` controls the parallelism for shuffle operations, such as joins and aggregations, in Spark SQL. By default, Spark sets this value to 200. However, in this example, it's explicitly set to 4, meaning that Spark will create 4 partitions for shuffle operations. Adjusting the number of shuffle partitions can optimize performance based on the size of the data and the resources available in the cluster.

### 2. Generating the sample datasets

As no sample datasets were provided, the case class DataGenerators (`src/main/scala/DataGenerators.scala`) was created to generate the sample datasets for Dataset A and B.
The datasets are generated in the `src/test/resources` folder. 

For Dataset A, considerations of having duplicate `detection_oid` values in this parquet file and having a data skew in data skew in one of the geographical locations in Dataset A were taken into account.
This was made possible by the `generateDetectionRecords` function in the DataGenerators class selecting values for the `detection_oid` column from a fixed list of values, ensuring that there will be repeated values in the `detection_oid` column.
The values for the `geographical_location_oid` column was selected from a fixed list of values, where half the list is made of a single Long value to increase the chances of the skewed value being picked.

### 3. Spark functions

Any functions that involved the use of Spark were placed in the `src/main/scala/Session.scala` file. This file contains `createSparkSession`, which allows the program to create a SparkSession where the datasets can be read and written.
The `readParquet` and `writeParquet` functions that read and write the parquet files respectively is also included in the file.

### 4. Transforming the datasets

In order to transform the generated dataset to the RDD data type, an encoder is required.
Hence, the object `Structures` in `src/main/scala/util/Structures.scala` contain the necessary encoders for each data type. 
Additional functions that transform the datasets are also included in the same file, such as `getTopXItemsDetected` to get the top X items detected in a given geographical location and `getTopXItemsDetectedInAllLocations` to get the top X items detected in all locations and the `broadcastHashJoin` function to join two datasets.
In order to work with skewed data, the object `Partitions` in this file contains the `partitionData` function that partitions the RDD based on the column that contains the skewed data. This was used to deal with the skewed data in Dataset A.

### 5. Utility functions

The `src/main/scala/util/Utils.scala` contains the `Utils` object, which contains static variables and functions that are used throughout the code. All variables and functions in this code can be used in any part of the code.

### 6. Testing the code

Unit and integration tests were created to test each function in the code. The tests are located in the `src/test/scala` folder. The tests aim to test the functions in the `Structures` object, the `Partitions` object, and the `Utils` object.


## Project Overview

### Built with

[Apache Spark](https://spark.apache.org/) - Data processing framework \
[IntelliJ Idea](https://www.jetbrains.com/idea/) - IDE \
[sbt](https://www.scala-sbt.org/) - Build tool
[Scala](https://www.scala-lang.org/) - Programming language

### Authors

**Jobelle Lee** - [themaxermister](https://github.com/themaxermister/DEND-Final_Capstone)



